set_confidence(make_name_obj(if_bool_string(FALSE, extract_given_str(get_remainder_tokens(EMPTY_TOK_LIST, filter_by_type(EMPTY_TOK_LIST, DEGREE))), extract_family_str(EMPTY_TOK_LIST)), if_bool_string(is_title(get_last_token(if_bool_tokenlist(is_salutation(EMPTY_TOKEN), slice_tokens(EMPTY_TOK_LIST, 4, TRUE), tokenize(raw_input)))), if_bool_string(FALSE, get_first_string(extract_middle_str(get_remainder_tokens(EMPTY_TOK_LIST, EMPTY_TOK_LIST))), EMPTY_STR), extract_salutation_str(if_bool_tokenlist(is_title(get_last_token(slice_tokens(slice_tokens(EMPTY_TOK_LIST, is_title(get_last_token(slice_tokens(slice_tokens(EMPTY_TOK_LIST, is_title(get_last_token(if_bool_tokenlist(is_salutation(EMPTY_TOKEN), slice_tokens(EMPTY_TOK_LIST, 4, TRUE), tokenize(to_lower(raw_input))))), is_salutation(get_last_token(tokenize(EMPTY_STR)))), 4, is_title(get_last_token(slice_tokens(filter_by_type(drop_first(EMPTY_TOK_LIST), identity_token_type(DEGREE)), index_of_type(EMPTY_TOK_LIST, identity_token_type(SUFFIX)), is_title(get_last_token(slice_tokens(EMPTY_TOK_LIST, index_of_type(EMPTY_TOK_LIST, identity_token_type(WORD)), FALSE))))))))), is_title(get_last_token(slice_tokens(remove_type(remove_type(slice_tokens(slice_tokens(drop_first(drop_first(EMPTY_TOK_LIST)), TRUE, is_title(get_last_token(if_bool_tokenlist(is_salutation(EMPTY_TOKEN), slice_tokens(EMPTY_TOK_LIST, 4, TRUE), tokenize(raw_input))))), has_comma(raw_input), FALSE), identity_token_type(identity_token_type(identity_token_type(SALUTATION)))), identity_token_type(INITIAL)), index_of_type(drop_first(filter_by_type(if_bool_tokenlist(has_comma(raw_input), filter_by_type(EMPTY_TOK_LIST, SALUTATION), filter_by_type(EMPTY_TOK_LIST, WORD)), DEGREE)), identity_token_type(identity_token_type(identity_token_type(identity_token_type(identity_token_type(PARTICLE)))))), TRUE)))), index_of_type(get_remainder_tokens(if_bool_tokenlist(is_title(get_last_token(slice_tokens(slice_tokens(EMPTY_TOK_LIST, is_title(get_last_token(slice_tokens(slice_tokens(EMPTY_TOK_LIST, is_title(get_last_token(filter_by_type(EMPTY_TOK_LIST, SUFFIX))), is_title(get_last_token(slice_tokens(remove_type(slice_tokens(slice_tokens(if_bool_tokenlist(is_salutation(EMPTY_TOKEN), EMPTY_TOK_LIST, EMPTY_TOK_LIST), TRUE, TRUE), is_title(get_last_token(slice_tokens(get_remainder_tokens(slice_tokens(if_bool_tokenlist(is_title(get_last_token(EMPTY_TOK_LIST)), EMPTY_TOK_LIST, get_remainder_tokens(EMPTY_TOK_LIST, drop_last(slice_tokens(filter_by_type(get_remainder_tokens(EMPTY_TOK_LIST, tokenize(trim(raw_input))), identity_token_type(PARTICLE)), is_salutation(EMPTY_TOKEN), FALSE)))), index_of_type(get_remainder_tokens(tokenize(trim(raw_input)), tokenize(extract_family_str(EMPTY_TOK_LIST))), identity_token_type(PARTICLE)), TRUE), EMPTY_TOK_LIST), index_of_type(EMPTY_TOK_LIST, identity_token_type(identity_token_type(identity_token_type(INITIAL)))), is_title(get_last_token(EMPTY_TOK_LIST))))), FALSE), identity_token_type(identity_token_type(identity_token_type(PUNCT)))), index_of_type(EMPTY_TOK_LIST, identity_token_type(SUFFIX)), is_title(EMPTY_TOKEN))))), FALSE, is_title(get_last_token(slice_tokens(filter_by_type(filter_by_type(EMPTY_TOK_LIST, PARTICLE), identity_token_type(DEGREE)), index_of_type(EMPTY_TOK_LIST, identity_token_type(SUFFIX)), is_title(get_last_token(slice_tokens(EMPTY_TOK_LIST, index_of_type(EMPTY_TOK_LIST, identity_token_type(WORD)), FALSE))))))))), is_title(get_last_token(get_remainder_tokens(EMPTY_TOK_LIST, tokenize(EMPTY_STR))))), index_of_type(get_remainder_tokens(tokenize(raw_input), tokenize(get_last_string(EMPTY_STR_LIST))), identity_token_type(PARTICLE)), is_title(get_last_token(EMPTY_TOK_LIST))))), filter_by_type(EMPTY_TOK_LIST, TITLE), drop_first(EMPTY_TOK_LIST)), tokenize(get_first_string(split_on_comma(EMPTY_STR)))), identity_token_type(identity_token_type(SALUTATION))), is_title(get_first_token(EMPTY_TOK_LIST))))), drop_last(drop_last(slice_tokens(filter_by_type(get_remainder_tokens(EMPTY_TOK_LIST, tokenize(trim(raw_input))), identity_token_type(PARTICLE)), is_salutation(get_first_token(EMPTY_TOK_LIST)), FALSE))), tokenize(raw_input)))), extract_middle_str(get_remainder_tokens(EMPTY_TOK_LIST, EMPTY_TOK_LIST)), extract_given_str(tokenize(raw_input)), extract_family_str(tokenize(raw_input)), extract_middle_str(tokenize(raw_input)), get_gender_from_salutation(get_first_token(tokenize(raw_input))), extract_suffix_list(EMPTY_TOK_LIST), extract_particles_list(tokenize(raw_input))), mul(0.96, 0.48))